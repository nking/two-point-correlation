-- need to revisit the portion of KDE critical density calculation
   that continues to add to weighted average for certain close spacing.
   -- consider inefficiently stored data that has large 
      x,y space between points within clusters and lower density
      than that outside of clusters.
      the current fixed values for some of the logic need revision.

-- more KDE:
   -- for the PMF/PDF of cluster points,
        -- currently, I'm using the surface density frequency curve
           used to calculate the critical density.
           -- may be able to derive a better underlying distribution
              to use for a PDF.
              -- it's an incomplete sparsely sampled GEV,
                 but I think the 3 independent parameters of the GEV
                 cannot be presumed to be recoverable from this sparse
                 sampling.
                 could use maximum likelihood and explore parameter space
                 but the parameter space is actually infeasibly large
                 (and the distr doesn't have the simplest relation of
                 peak to location parameter for example to determine
                 location parameter first).
        -- need a fast way to calculate the surface density of points 
           within a cluster.
           since the approach is pairwise, need only determine the
           nearest neighbor within the cluster to get a surface density.
        -- would like to retain the portion of the surface density
           frequency errors arising from the use of a kernel,
           so need to add the wavelet coefficients to the DensityHolder
           for this use if the KDE stats are requested.
           -- for each point, the error is the sum of the coefficient
              values of that point added in quadrature because each is
              the result of kernel operation of size h bandwith upon
              the region of that point.
              then multiplied by the point count,
              then divied by the normalization to result in errors
              in terms of the probabilities.
              these are just one component of the true error though.

-- rewrite the one dbscan image unit test if missing since refactoring

-- consider changing default option to KDE

-- add the Knearest neighbors to the shared library
   and 1D wavelets

   -- implement the knn density curve using
      the knn imported into the shared library
   -- (bishop 2006 pg 147 and before has examples showing
       representation of histogram, kde, knn for a dataset.
       need to explore determining k, V, and bin size for
       large range of data

-- this project needs 2 different builds and the explanation
   added to both added to README.txt and a note considering 
   mvn to make version control clearer.
   (1) build including shared and trove
   (2) build excluding shared and trove
   -- add the dependency information to the build missing the
      shared class files

-- create ability for multiple dimensions
   -- that is n^2/2 number of dts and/or knn
-- need improvements in hist...

-- update the javadocs
-- update coverage reports
-- update the wiki github io web pages

-- redo a couple of "other" snapshots on two-pt-correl web page:
   the dbscan and the curved arrow from the finnish data set
