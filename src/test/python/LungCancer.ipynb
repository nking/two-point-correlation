{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143d846a-58eb-4d35-bdd5-dcbe62de3379",
   "metadata": {},
   "source": [
    "The lung cancer data used in this notebook\n",
    "was published in :\n",
    "          Hong, Z.Q. and Yang, J.Y. \"Optimal Discriminant Plane for a Small\n",
    "          Number of Samples and Design Method of Classifier on the Plane\",\n",
    "          Pattern Recognition, Vol. 24, No. 4, pp. 317-324, 1991.\n",
    "        - Donor: Stefan Aeberhard, stefan _at_ coral.cs.jcu.edu.au\n",
    "        - Date : May, 1992\n",
    "\n",
    "    32 lines\n",
    "    57 (1 class attribute, 56 predictive)\n",
    "    attribute 1 is the class label.\n",
    "        - All predictive attributes are nominal, taking on integer\n",
    "          values 0-3\n",
    "    Missing Attribute Values: Attributes 5 and 39 (*)\n",
    "    (5 lines have missing data)\n",
    "\n",
    "    9. Class Distribution:\n",
    "        - 3 classes,\n",
    "                1.)     9 observations\n",
    "                2.)     13     \"\n",
    "                3.)     10     \"\n",
    "\n",
    "     reading the data into:\n",
    "      y = column data 0, the class\n",
    "      x = columns 1 through 56\n",
    "\n",
    "The notebook is to explore fitting models that generalize well to test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d48ea-63c3-4ff6-90f3-4b7ab6247c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c2f62-23ba-4f83-b5fa-4823a6ee37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' read in the data '''\n",
    "csv_path = os.getcwd() + \"/../../test/resources/ucl_ml_datasets/lung+cancer/lung-cancer_data.txt\"\n",
    "\n",
    "nRows = 32\n",
    "nCols = 57\n",
    "\n",
    "# labels are 1, 2, or 3\n",
    "labels = []\n",
    "labels_no_missing_data = []\n",
    "# attributes have values  0-3\n",
    "utility = []\n",
    "utility_no_missing_data = []\n",
    "missingValue = None # catboost understands N/A, NAN, NoneN/A, NAN, None\n",
    "\n",
    "with open(csv_path) as fp:\n",
    "    i = 0\n",
    "    for line in fp:\n",
    "        missing_data = False\n",
    "        line = line.rstrip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        items = line.rsplit(\",\")\n",
    "        x_i = []\n",
    "        for j in range(1, len(items)):\n",
    "            if items[j] == \"?\":\n",
    "                x_i.append(missingValue)\n",
    "                missing_data = True\n",
    "            else:\n",
    "                x_i.append(int(items[j]))\n",
    "        labels.append(int(items[0]) - 1)\n",
    "        utility.append(x_i)\n",
    "        if not missing_data:\n",
    "            labels_no_missing_data.append(int(items[0]) - 1)\n",
    "            utility_no_missing_data.append(x_i)\n",
    "        i += 1\n",
    "    fp.close()\n",
    "\n",
    "y = np.array(labels_no_missing_data)\n",
    "X = np.array(utility_no_missing_data)\n",
    "y0 = np.array(labels)\n",
    "X0 = np.array(utility)\n",
    "\n",
    "print(f'X.shape={X.shape}, y.shape={y.shape}\\n')\n",
    "print(f'X0.shape={X0.shape}, y0.shape={y0.shape}\\n')\n",
    "\n",
    "print(f'done reading in data\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909a54d-06df-4d15-995c-87b1c76d9439",
   "metadata": {},
   "source": [
    "## A look at SVD based projections of the data separated by labels, reduced to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb50c11-73cd-4011-bde8-26a2e08576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "# can see non-linearity in the individual label projections,\n",
    "# so should consider a  nonlinear dimensionality reduction method.\n",
    "p=2\n",
    "clr = {0:'red', 1: 'cyan', 2:'black'}\n",
    "vps = []\n",
    "projs = []\n",
    "for i in range(3):\n",
    "    indices = y == i\n",
    "    lmnd = y[indices]\n",
    "    umnd = X[indices]\n",
    "    U, s, Vh = linalg.svd(umnd)\n",
    "    print(f'{i} s={s}\\n')\n",
    "    vp = Vh.T.copy()[:, 0:p]\n",
    "    projected = np.matmul(umnd, vp)\n",
    "    plt.scatter(projected[:,0], projected[:,1], s=10, color=clr[i])\n",
    "    plt.show()\n",
    "    vps.append(vp)\n",
    "    projs.append(projected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed6433-7009-4479-bc25-4e3bdcc1efab",
   "metadata": {},
   "source": [
    "## A look at SVD based projection of data, reduced to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129a131-86ff-4e0b-8606-4a1729dde68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variance in the first singular value is very large compared to the remaining,\n",
    "# but the projection does not show clear segmentation\n",
    "\n",
    "U, s, Vh = linalg.svd(X)\n",
    "print(f's={s}\\n')\n",
    "'''s=[72.13455958  9.46386207  7.72091041  6.75346809  6.47903299  6.01602643\n",
    "  5.55968427  5.37546843  4.95311595  4.86989157  3.94966496  3.81784701\n",
    "  3.66035581  3.54406066  3.25177668  3.05968404  2.86141846  2.60421373\n",
    "  2.52336053  2.24686865  2.09645093  2.02801034  1.63697713  1.45533736\n",
    "  1.39472266  1.12843462  0.78338147]'''\n",
    "\n",
    "vp = Vh.T.copy()[:, 0:p]\n",
    "projected = np.matmul(X, vp)\n",
    "for i in range(3):\n",
    "    indices = y == i\n",
    "    xp = projected[:,0]\n",
    "    xp = xp[indices]\n",
    "    yp = projected[:,1]\n",
    "    yp = yp[indices]\n",
    "    plt.scatter(xp, yp, s=5, color=clr[i])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb896b6-f347-469b-81f6-82aa22747052",
   "metadata": {},
   "source": [
    "To use CatBoost or LightGBM below, we split the data into train and test.\n",
    "If we had enough data we would split it into train, dev, and test.\n",
    "\n",
    "For such a small dataset, we use 80% train, 20% test.\n",
    "We have 3 labels, so we choose 80% from each for train, and the rest for test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a10118-0131-438b-80ea-208b3ba46ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement k-cross validation here, that is, generate k (train,test) datasets\n",
    "\n",
    "_X = X0 # X0, y0 replace missing values with None, which CatBoost handles\n",
    "_y = y0\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for i in range(3):\n",
    "    indices = _y == i\n",
    "    xp = _X[indices]\n",
    "    tr_indices = np.random.choice(xp.shape[0], int(xp.shape[0] * 0.8), replace=False)\n",
    "    te_indices = np.array(list(set([i for i in range(len(xp))]) - set(tr_indices)))\n",
    "    x_train.extend(xp[tr_indices].tolist())\n",
    "    x_test.extend(xp[te_indices].tolist())\n",
    "    for _ in range(xp[tr_indices].shape[0]):\n",
    "        y_train.append(i)\n",
    "    for _ in range(xp[te_indices].shape[0]):\n",
    "        y_test.append(i)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "print(f'X.shape={_X.shape}, lengths={len(_X)}, {len(_X[0])}\\n')\n",
    "print(f'x_train.shape={x_train.shape}, lengths={len(x_train)}, {len(x_train[0])}\\n')\n",
    "print(f'x_test.shape={x_test.shape}, lengths={len(x_test)}, {len(x_test[0])}\\n')\n",
    "print(f'len(x_train)={len(x_train)}, len(y_train)={len(y_train)}\\n')\n",
    "print(f'len(x_test)={len(x_test)}, len(y_test)={len(y_test)}\\n')\n",
    "print(f'done splitting into train and test\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cd1e3-872f-483e-85c3-4012f2b34531",
   "metadata": {},
   "source": [
    "## A look at LocallyLinearEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631417c-353c-4150-82e6-b837f9c0bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "p = 2\n",
    "lle = LocallyLinearEmbedding(n_components=p)\n",
    "X_transformed = lle.fit_transform(X, y)\n",
    "print(f'X_transformed.shape={X_transformed.shape}\\n')\n",
    "\n",
    "#lle.get_feature_names_out()\n",
    "#lle.get_params()\n",
    "\n",
    "#plt.scatter(X_transformed[:,0], X_transformed[:,1], s=5, color=clr[i])   \n",
    "#plt.show()\n",
    "\n",
    "# show the different labels as different colors\n",
    "for i in range(3):\n",
    "    indices = y == i\n",
    "    xp = X_transformed[:,0]\n",
    "    xp = xp[indices]\n",
    "    yp = X_transformed[:,1]\n",
    "    yp = yp[indices]\n",
    "    plt.scatter(xp, yp, s=5, marker='o', color=clr[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985cf5b-90b5-4ff4-816f-03e7d9c7efb2",
   "metadata": {},
   "source": [
    "## A look at T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64560e5b-04a1-40d8-9f00-5490413af52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "#init='random'\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='pca', \n",
    "                  method='exact', perplexity=2).fit_transform(X)\n",
    "X_embedded.shape\n",
    "\n",
    "for i in range(3):\n",
    "    indices = y == i\n",
    "    xp = X_embedded[:,0]\n",
    "    xp = xp[indices]\n",
    "    yp = X_embedded[:,1]\n",
    "    yp = yp[indices]\n",
    "    plt.scatter(xp, yp, s=5, marker='o', color=clr[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae55d69-6908-483b-b1ea-edd45c89a62e",
   "metadata": {},
   "source": [
    "## A look at CatBoost, which is supervised machine learning (uses the labels)\n",
    "\n",
    "Helpful in using CatBoost and Shap was this tutorial \"CatBoost regression in 6 minutes\" by Simon Thiesen: https://towardsdatascience.com/catboost-regression-in-6-minutes-3487f3e5b329\n",
    "\n",
    "CatBoost is a gradient boosting machine learning library to solve for categorical features\n",
    "using oblivious decision trees.  It can handle numerical, categorical, and text feature data.\n",
    "\n",
    "A few notes about Boosting and ensemble methods from \n",
    "Kevin Murphy's 1st book \"Probalistic Machine Learning: An Introduction\" and from Stanford CS 229 Machine Learning lecture notes.\n",
    "\n",
    "Ensemble learning is averaging multiple models to reduce the bias and/or variance.\n",
    "For binary classification, majority vote is often used instead of averaging.\n",
    "Note that weighted averaging is called \"Stacking\".\n",
    "\n",
    "Ensemble learning combine a set of weak learner to create a strong learner that obtains better performance than a single one.\n",
    "\n",
    "Definitions of weak and strong learners:\n",
    "   * Weak learners \n",
    "       a.k.a. Weak Classifiers: \n",
    "       a classifier that achieves slightly better than 50 percent accuracy.\n",
    "       these are models that perform slightly better than random guessing.\n",
    "   * Strong learners: \n",
    "       models that have arbitrarily good accuracy.\n",
    "\n",
    "There are many ways to ensemble models in machine learning, \n",
    "such as Bagging, Boosting, and Stacking.\n",
    "\n",
    "Boosting decreases bias, not variance.\n",
    "Boosting is more suitable for data with low variance, high bias, and high\n",
    "noise, as it can reduce underfitting and increase accuracy.\n",
    "\n",
    "As an aside, the subnetwork referred to as a \"Residual Block\" is an example used in \n",
    "Stacking.\n",
    "\n",
    "Also as an aside, Bagging (Bootstrap Aggregation) is a method of merging the same type of predictions.\n",
    "Bagging decreases variance, not bias, and solves over-fitting issues in a\n",
    "model.   The RandomForest method is an example of Bagging.\n",
    "\n",
    "More regarding CatBoost from [dataaspirant](https://dataaspirant.com/catboost-algorithm/)\n",
    "and (towardsdatascience)[https://towardsdatascience.com/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14]:\n",
    "\n",
    "The boosting tree features used in a split are chosen to maximize the split score accuracy which \n",
    "includes a penalty function.\n",
    "The boosting trees are oblivious and balanced trees wherein the same split criterion \n",
    "is used across a level.  The feature-split pair chooses a leaf and this is repeated until\n",
    "the leaves reach the tree depth.\n",
    "\n",
    "And from (Bard)[https://bard.google.com] when asked to summarize leaf growth in CatBoost:\n",
    "\n",
    "Leaf growth in CatBoost: A summary\n",
    "\n",
    "Leaf growth in CatBoost refers to the process of building the decision tree at the heart of the model. Here's a breakdown of the key points:\n",
    "\n",
    "Tree growing methods:\n",
    "\n",
    "* SymmetricTree: Default approach, builds levels sequentially with fixed splits for all leaves. Fast and often effective, but less flexible than other options.\n",
    "* Depthwise: Expands by splitting non-terminal leaves with the best loss improvement. More adaptable but not as fast as SymmetricTree.\n",
    "* Lossguide: Grows leaf by leaf, choosing the non-terminal with the highest gain. Most flexible but not supported for certain analysis methods.\n",
    "\n",
    "Splitting criteria:\n",
    "\n",
    "* Minimizes a penalized loss function like cross-entropy or MAE. Penalty terms like L2 regularization prevent overfitting.\n",
    "* Best split is chosen based on the improvement it brings to the overall loss.\n",
    "\n",
    "Leaf size limitations:\n",
    "\n",
    "* min_data_in_leaf: Minimum number of samples allowed in a leaf, controls complexity and prevents noise impact.\n",
    "* max_leaves: Sets the maximum number of leaves in the tree, another way to control model complexity.\n",
    "\n",
    "Additional factors:\n",
    "\n",
    "* Bootstrap aggregation (bagging): Ensembles multiple trees (built on different subsets of data) for better accuracy and robustness.\n",
    "* Tree depth: Deeper trees can capture more complex relationships but risk overfitting.\n",
    "\n",
    "Understanding leaf growth is crucial for:\n",
    "\n",
    "* Tuning CatBoost parameters effectively.\n",
    "* Interpreting the decision tree's structure and decision rules.\n",
    "Analyzing model complexity and preventing overfitting.\n",
    "\n",
    "Further resources:\n",
    "\n",
    "CatBoost documentation on tree growing policies: https://catboost.ai/docs/concepts/parameter-tuning\n",
    "Parameter tuning guide: https://catboost.ai/docs/concepts/parameter-tuning\n",
    "Understanding the CatBoost algorithm: https://medium.com/@harshitaaswani2002/practical-applications-of-catboost-in-data-science-a99f6ff12d00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc2cbb-a12d-499f-a8d6-89a27bda19a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "import shap\n",
    "\n",
    "# there are 56 features\n",
    "nd = int(np.sqrt(x_train.shape[1]))\n",
    "\n",
    "cat_features = [0,1,2] #this is needed when they're categorical, e.g. ['a','b','c']\n",
    "loss_f = 'MultiClass'\n",
    "#loss_f = 'logloss' #default\n",
    "#l2_leaf_reg = 8 #default is 3\n",
    "#depth=6 #default\n",
    "model = CatBoostClassifier(learning_rate=0.03, loss_function=loss_f, metric_period=100,\n",
    "                          classes_count=3)\n",
    "\n",
    "#bootstrap_type='Regularization'\n",
    "# helpful: https://towardsdatascience.com/catboost-regression-in-6-minutes-3487f3e5b329\n",
    "\n",
    "# CAVEAT: larger depth can lead to overfitting\n",
    "grid_params = {'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 4, 6, 8],\n",
    "        'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "model.grid_search(grid_params, x_train, y_train, verbose=True, plot=True)\n",
    "#model.fit(x_train, y_train, verbose=True, plot=True)\n",
    "\n",
    "print(f'trained model params={model.get_params()}')\n",
    "\n",
    "print(f'done training CatBoostClassifier\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced9a51-35c7-4ef6-bafa-e284fb1a7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_importance = model.feature_importances_.argsort()\n",
    "print(f'sorted_feature_importance={sorted_feature_importance}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7af5e-a9a5-4cff-9d44-85ef06d277b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "y_proba_test = model.predict_proba(x_test)\n",
    "assert(len(y_pred_test) == len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    print(f'true={y_test[i]}, predicted={y_pred_test[i]},  prob={y_proba_test[i]}')\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print('RMSE: {:.2f}'.format(rmse))\n",
    "print('R2: {:.2f}'.format(r2))\n",
    "# this from Google generative AI:\n",
    "print(f'Substantial, moderate and weak R^2 scores respectively are 0.75, 0.50, and 0.25\\n')\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "shap.summary_plot(shap_values, x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba127c5-f3c9-45bc-8062-75d4a72ddea8",
   "metadata": {},
   "source": [
    "## CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492fd23-7518-4987-85c3-e57e34578f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoost, CatBoostRegressor\n",
    "\n",
    "# used to compare models, but can be run for a single model:\n",
    "'''\n",
    "param = {'iterations':500, 'learning_rate':0.03, 'loss_function':loss_f, 'metric_period':100,\n",
    "                          'classes_count':3}\n",
    "model = CatBoost(param)\n",
    "model.fit(x_train, y_train, verbose=True, plot=True)\n",
    "\n",
    "print(f'done training CatBoost\\n')\n",
    "\n",
    "preds_class = model.predict(x_test, prediction_type='Class')\n",
    "preds_proba = model.predict(x_test, prediction_type='Probability')\n",
    "preds_raw_vals = model.predict(x_test, prediction_type='RawFormulaVal')\n",
    "print(\"Class\", preds_class)\n",
    "print(\"Proba\", preds_proba)\n",
    "print(\"Raw\", preds_raw_vals)\n",
    "\n",
    "print(f'done testing with CatBoost\\n')\n",
    "'''\n",
    "'''\n",
    "loss function choices:\n",
    " |      'RMSE' <-- default\n",
    " |      'MAE'  <-- Mean Absolute  Error\n",
    " |      'Quantile:alpha=value'\n",
    " |      'LogLinQuantile:alpha=value'\n",
    " |      'Poisson'\n",
    " |      'MAPE'   <-- Mean Absolute Percentage Error\n",
    " |      'Lq:q=value'\n",
    " |      'SurvivalAft:dist=value;scale=value'\n",
    " '''\n",
    "loss_f = 'RMSE'\n",
    "\n",
    "nd = int(x_train.shape[1]/3)\n",
    "\n",
    "model = CatBoostRegressor(learning_rate=0.03, loss_function=loss_f, metric_period=100, verbose=True)\n",
    "\n",
    "#CAVEAT: large tree depth can lead to overfitting\n",
    "grid_params = {'learning_rate': [0.03, 0.1],\n",
    "        'depth': [2, 3, 4, 6],\n",
    "        'l2_leaf_reg': [0.01, 0.1, 1, 3, 6]}\n",
    "model.grid_search(grid_params, x_train, y_train, verbose=True, plot=True)\n",
    "#model.fit(x_train, y_train, plot=True)\n",
    "\n",
    "sorted_feature_importance = model.feature_importances_.argsort()\n",
    "print(f'sorted_feature_importance={sorted_feature_importance}\\n')\n",
    "\n",
    "print(f'trained model params={model.get_params()}')\n",
    "\n",
    "print(f'done training CatBoostRegressor\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c0679-5359-4fd0-8d8e-0947e1c90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_type='RawFormulaVal' #default\n",
    "prediction_type='Probability'\n",
    "y_pred_test = model.predict(x_test, prediction_type='RawFormulaVal')\n",
    "y_proba_test = model.predict(x_test, prediction_type='Probability')\n",
    "assert(len(y_pred_test) == len(y_test))\n",
    "for i in range(len(y_test)):\n",
    "    print(f'true={y_test[i]}, predicted={y_pred_test[i]},  prob={y_proba_test[i]}')\n",
    "\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print('RMSE: {:.2f}'.format(rmse))\n",
    "print('R2: {:.2f}'.format(r2))\n",
    "# this from Google generative AI:\n",
    "print(f'substantial, moderate and weak R^2 scores respectively are 0.75, 0.50, and 0.25\\n')\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(x_test)\n",
    "shap.summary_plot(shap_values, x_test)\n",
    "\n",
    "print(f'done with prediction on test dataset using CatBoostRegressor\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d66086-c5e4-4389-92d4-33db41f2f1c2",
   "metadata": {},
   "source": [
    "paused here... lightgbm next"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
