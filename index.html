<!DOCTYPE html>
<html lang="en-us">
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-11146214-7"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-11146214-7');
    </script>

    <meta charset="UTF-8">
    <title>two-point-correlation by nking</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">two-point-correlation</h1>
      <h2 class="project-tagline">Unsupervised Density Based Clustering</h2>
      <a href="https://github.com/nking/two-point-correlation" class="btn">View on GitHub</a>
      <a href="https://github.com/nking/two-point-correlation/raw/master/dist/com.climbwithyourfeet.clustering.jar" class="btn">Download jar file</a>
    </section>

    <section class="main-content">
      <p><img src="images/snapshot.001.png" width="250" height="250"> <img src="images/snapshot.002.png" width="250" height="250"> <img src="images/snapshot.004.png" width="250" height="250"></p>

<p>This project's algorithm is an <b>unsupervised density based clustering algorithm</b>.
    The algorithm does not require prior knowledge of the number of clusters, nor does it require a separation distance
    for association of points.  The algorithm finds convex and non-convex cluster shapes in a statistically based manner
    that is reproducable. The algorithm learns the association distance of cluster membership by
    finding the background (surface) density distribution using histograms of the point separations.
    Once the critical distance for point association is determined, the algorithm finds points associated with
    one another, that is, clusters.
  </p>

<p>
    A few more details on the background surface density:
    The location of the background points in 2-dimensional space are likely an exponential distribution such as
    poisson or gaussian which means that a histogram formed from the 2-point separations can be fit by
    a Generalized Extreme Value (GEV) curve.  The peak gives us the background density and separations smaller than
    that are clustered.  One often assumes that the clustering scale should be 2.5 times the background noise scale,
    that is, the critical separation should be 2-3 times smaller than the separation found for background points.
</p>

<p> The algorithm runtime complexity is roughly <b>O(N X log<sub>2</sub>(N))</b>.  A previous version used an
    <b>O(N)</b> distance transform for the point separations, but the dataset must be dense for that
    to be an advantage.
</p>

<!--constructs a distance transform for every non-point relative to the nearest point in the dataset.
Histograms are formed from the inverse square root of the distance transform and the first peaks in the histogram
are fit to derive the critical density.  The critical separation between 2 points is then calculated from the critical
density and a factor above background.  Any points closer to one another than the critical separation are
within clustering distance of one another.-->

<p>
    Several dataset examples are shown below.
The scatter plots and histograms below use <a href="http://d3js.org">d3 js</a></p>

<p>
The same process can be used with the distances calculated from a brute force O(n^2) approach.
    This is demonstrated on a subset of the Amazon Fine Food Reviews dataset, which is very large, but sparse,
    that is a utility matrix of users versus products score is very spase.
    For this I put the java and python code in the test directory.  The python code uses
    pytorch's sparse matrix and sparse linear algebra and matplotlib.pyplot. The java code uses a cluster finder from
    this project and xchart.
</p>

        <p>Usage as an API:</p>

<pre>
    To use the code with default settings:

    DTClusterFinder clusterFinder = new DTClusterFinder(points,
        imageWidth, imageHeight);

	    clusterFinder.calculateCriticalDensity();

    // or, set density instead of calculate (for use in pca or lda, for example):
    //clusterFinder.setCriticalDensity(dens);

    clusterFinder.findClusters();

    int nGroups = clusterFinder.getNumberOfClusters();

    List&lt;Set&lt;PairInt&gt;&gt; groupList = new ArrayList&lt;Set&lt;PairInt&gt;&gt;();
    for (int k = 0; k < nGroups; ++k) {
        Set&lt;PairInt&gt; set = clusterFinder.getCluster(k);
        groupList.add(set);
    }
</pre>

<hr>

<blockquote>
<p>The citation for use of this code in a publication is:</p>

<pre>
<pre>
For use before Sep, 2015
    http://code.google.com/p/two-point-correlation/, 
    Nichole King,  "Unsupervised Clustering Based Upon Voids in Two-Point Correlation". March 15, 2013.

else
    https://github.com/nking/two-point-correlation
    Nichole King,  "Unsupervised Density Based Clustering". September 25, 2015.

The code is licensed under the MIT license, usable for commercial and non-commercial purposes:
    http://opensource.org/licenses/mit-license.php
    see LICENSE.txt in this project
</pre>
</blockquote>

<p>The previous version of this code is presented in the <a href="gev.html">previous web page</a>.

<p>Note that I wrote the core algorithm in this work (without the automated density calculation) several years ago and the results were part of a publication.
What was published were the results from this algorithm used as input for another algorithm that requires 
knowledge of association radius in order to work.   The algorithm that used my algorithm's output required a parameter that was not derivable from the other algorithm's use alone.
Similarly, "k-means clustering" requires knowledge of the number of clusters before use.  Delaunay Triangulation is useful if there are no background points within a dataset, that is all points will be members of a group, and if groups do not have non-convex shapes.  KDTrees are useful as a nearest neighbor algorithm, but its use in determining clusters would still require as input, an association radius.
The core of the algorithm here is was what I needed to create awhile back for work applied to galaxy surveys.  The addition published here is automation of the background determination and large improvements of the
overall algorithm.  I'm using it in a computer vision project for segmentation.</p>

<blockquote>

<hr>
<h2>
<a id="sparsely-populated-background" class="anchor" href="#sparsely-populated-background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sparse Clusters and no Background</h2>

<p><img src="images/snapshot_sparse_01_dt.png" width="300"></p>
<hr>

<h2>
<a id="moderately-populated-background" class="anchor" href="#moderately-populated-background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Moderately Dense Clusters and a non-clustered Background</h2>

<p><img src="images/snapshot_clusters_with_background_01_dt.png" width="300"></p>
<hr>

<h2>
<a id="non-convex-morphology-clusters" class="anchor" href="#non-convex-morphology-clusters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-Convex Morphology Clusters</h2>

<!--
<p><img src="images/wikipedia_dbscan.png"></p>
-->

<p>The cluster "shape" datasets collected at <a href="http://cs.joensuu.fi/sipu/datasets/">http://cs.joensuu.fi/sipu/datasets/</a> are fit here.</p>

<p>These clusters were found with the default algorithm settings:</p>
<img src="images/snapshot_shapes_01_dt.png" width="300">
<img src="images/snapshot_shapes_02_dt.png" width="300">
<img src="images/snapshot_shapes_03_dt.png" width="300">
<img src="images/snapshot_shapes_04_dt.png" width="300">
<img src="images/snapshot_shapes_05_dt.png" width="300">
<img src="images/snapshot_shapes_06_dt.png" width="300">
<img src="images/snapshot_shapes_07_dt.png" width="300">
<img src="images/snapshot_shapes_08_dt.png" width="300">

<hr>
<h2>
<a id="no-clusters" class="anchor" href="#no-clusters" aria-hidden="true"><span class="octicon octicon-link"></span></a>No Clusters</h2>
<p>No clusters are found where there are none.  Note that the higher density point images begin to generate a small percentage of clusters due to crowding from random noise.</p>
<img src="images/snapshot_no_clusters_01_dt.png" width="300">
<img src="images/snapshot_no_clusters_02_dt.png" width="300">
<img src="images/snapshot_no_clusters_03_dt.png" width="300">
<img src="images/snapshot_no_clusters_04_dt.png" width="300">
<img src="images/snapshot_no_clusters_05_dt.png" width="300">
<img src="images/snapshot_no_clusters_06_dt.png" width="300">
<hr>

    <hr>
    <h2>
        <a id="amazon-food-reviews" class="anchor" href="#amazon-food-reviews"
           aria-hidden="true"><span class="octicon octicon-link">
        </span></a>Clustering in 2 subsets of the Amazon Fine Food Reviews Dataset</h2>

    <p><b>The utility matrix of user scores for products projected to 2-dimensions using the largest eigvenvalue eigenvectors:</b></p>
    <p><img src="images/amazon_fine_food_reviews_projected.png" width="300">
    </p>
    A few notes of what the projection shows:
    <ul>
        <li>
        projections near 0,0 are users who reviewed 1 product, but those products have been reviewed by many users.
        the ratings may be different.</li>
        <li>
        projections to the left of center with y coordinate being near 0
        are users with many reviews who have products in common
        </li><li>
        projections above center with x being near 0
        in clusters are users with a couple of reviews and those users have a product in common.
    </li><li>
        projections to left and above center and not in clusters
        are users with several reviews having no products in common
    </li><li>
        projections to left and above center that are in clusters
        are users with a few reviews that have 1 product in common.  a spot check shows that
        all have the same high score for that product in common.
    </li><li>
        projections to left and below center that are in clusters
        are users with a few reviews that have 1 product in common.  a spot check shows that
        all have the same high score for that product in common.
    </li>
</ul>


    <p><b>A zoom in of the projected utility matrix:</b></p>
    <p><img src="images/amazon_fine_food_reviews_projected_zoom.png" width="300">
    </p>
    <p>A subset of the projected utility matrix, used to determine critical density for clustering and same subset
        with clusters plotted:</p>
    <p><img src="images/amazon_fine_food_reviews_projected_subset_1.png" width="300">
    <img src="images/amazon_fine_food_reviews_projected_subset_1_clusters.png" width="300">
    </p>
    <p>Another subset of the projected utility matrix.  The clustering uses the critical density determined from subset 1:</p>
    <p><img src="images/amazon_fine_food_reviews_projected_subset_2.png" width="300">
        <img src="images/amazon_fine_food_reviews_projected_subset_2_clusters.png" width="300">
    </p>
    <hr>


    <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/nking/two-point-correlation">two-point-correlation</a> is maintained by <a href="https://github.com/nking">nking</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>
  
  </body>
</html>
